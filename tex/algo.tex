\section{Algorithms}
% ...
\begin{minipage}{\textwidth}
  \begin{algorithm}[H]
  \DontPrintSemicolon
  \SetAlgoLined
  \SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
  \Input{$\mathcal{A}^\tau, \mathcal{B}^\tau, b, u_0, \nu_1, \nu_{\ASP}$}
  \Output{$u_k$}
  \BlankLine

    $k \gets 0$\;
    \While{$k \leq \nu_{\ASP}$ \textbf{and} not convergence}{
      $u_k \gets \mbox{\texttt{smoother}}_1(\mathcal{A}^\tau,b,u_k,\nu_1)$ \tcp*{Apply Gauss-Seidel smoother}  
      $d_k \gets b - \mathcal{A}^\tau u_k$      \tcp*{Compute the defect}
      $u_c \gets \mathcal{B}^\tau d_k$          \tcp*{ASP correction}
      $u_k \gets u_k + u_c$      \tcp*{Update the solution}
      $k \gets k + 1$\;
    }

  \caption{{\sc ASP}: Auxiliary Space Preconditioning for $\bm{V}_h(\textbf{curl},\Omega)$}
  \label{algo:asp-1}
  \end{algorithm} 
\end{minipage}
% ...

\subsection{Fast Diagonalization method}
In order to device a fast solver the Poisson and Laplace problems, we choose to follow the work of Sangalli and Tani \cite{sangalli2016}, we describe in the sequel the fast diagonalization method in the case of Isogeometric Analysis. This method was first introduced in \cite{lynch1964}.
\\
For the sack of simplicity, we shall consider the following Laplace problem, 
\begin{equation}
  \begin{cases}
  - \nabla^2 u + \tau u = f, \quad \Omega \\ 
  u=0, \quad \partial\Omega
  \end{cases}
  \label{eq:laplace}
\end{equation}
The Poisson problem and its solver shall be retrieved with $\tau=0$.
After discretizing the Laplace problem, we get the following linear system
\begin{equation}
  \mathcal{L}_{\tau} x := \left( K_1 \otimes M_2 \otimes M_3 +  M_1 \otimes K_2 \otimes M_3 + M_1 \otimes M_2 \otimes K_3 + \tau M_1 \otimes M_2 \otimes M_3 \right) x = b 
  \label{eq:laplace-kron}
\end{equation}
We first consider the generalized eigendecompositions problems
\begin{equation}
  K_1 U_1 = M_1 U_1 D_1, \quad  
  K_2 U_2 = M_2 U_2 D_2, \quad 
  K_3 U_3 = M_3 U_3 D_3,
  \label{eq:generalized-eigen-decomp}
\end{equation}
where $D_1$, $D_2$ and $D_3$ are diagonal matrices such that
\begin{equation}
  U_1^T M_1 U_1 = I_1, \quad  
  U_2^T M_2 U_2 = I_2, \quad  
  U_3^T M_3 U_3 = I_3
%  \label{}
\end{equation}
Therefor, (\ref{eq:laplace-kron}) can be written as
\begin{equation}
  \left( U_1 \otimes U_2 \otimes U_3 \right)^{-1} 
  \left( D_1 \otimes I_2 \otimes I_3 +  I_1 \otimes D_2 \otimes I_3 + I_1 \otimes I_2 \otimes D_3 + \tau I_1 \otimes I_2 \otimes I_3 \right) 
  \left( U_1 \otimes U_2 \otimes U_3 \right)^{-T} 
  x = b 
  \label{eq:laplace-kron-fact}
\end{equation}
The direct solver for the Laplace problem (\ref{eq:laplace-kron}), is then given by the following algorithm, where we omit the initialization step achieved by solving the generalized eigendecompositions in (\ref{eq:generalized-eigen-decomp}):
% ...
\begin{algorithm}[ht]
\DontPrintSemicolon
\SetAlgoLined
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{$\mathcal{L}_{\tau}, b$}
\Output{$x$}
\BlankLine
$\tilde{b} \gets \left( U_1 \otimes U_2 \otimes U_3 \right) b$ \; 
$\tilde{x} \gets \left( D_1 \otimes I_2 \otimes I_3 +  I_1 \otimes D_2 \otimes I_3 + I_1 \otimes I_2 \otimes D_3 + \tau I_1 \otimes I_2 \otimes I_3 \right)^{-1} \tilde{b}$ \; 
$x \gets \left( U_1 \otimes U_2 \otimes U_3 \right)^T \tilde{x}$ \; 

\caption{\texttt{fast\_diag}: Fast diagonalization method for Laplace problem}
\end{algorithm} 
% ...
We consider now the vector Laplace problem
\begin{equation}
  \begin{cases}
  - \nabla^2 \mathbf{u} + \tau \mathbf{u} = \mathbf{f}, \quad \Omega \\
  \mathbf{u}=0, \quad \partial\Omega
  \end{cases}
  \label{eq:laplace-vector}
\end{equation}
Which can be written in a matrix form as
\begin{equation} 
  \bm{\mathcal{L}}_{\tau} =
  \begin{bmatrix}
   \mathcal{L}_{\tau} &                  0 & 0  \\
                    0 & \mathcal{L}_{\tau} & 0  \\
                    0 &                  0 & \mathcal{L}_{\tau}  
  \end{bmatrix}.
  \label{eq:laplace-vector-matrix-form}
\end{equation}
Therefor, a fast solver for the vector Laplace problem (\ref{eq:laplace-vector}) can be given by the following algorithm
% ...
\begin{algorithm}[H]
\DontPrintSemicolon
\SetAlgoLined
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{$\bm{\mathcal{L}}_{\tau}, b$}
\Output{$x$}
\BlankLine

  $b_1, b_2, b_3 \gets \texttt{unfold}(b)$ \; 
  $x_1 \gets \texttt{fast\_diag}(\mathcal{L}_{\tau}, b_1)$ \; 
  $x_2 \gets \texttt{fast\_diag}(\mathcal{L}_{\tau}, b_2)$ \; 
  $x_3 \gets \texttt{fast\_diag}(\mathcal{L}_{\tau}, b_3)$ \; 
  $x \gets \texttt{fold}(x_1, x_2, x_3)$ \; 

\caption{\texttt{fast\_diag}: Fast diagonalization method for the vector Laplace problem}
\end{algorithm} 
% ...

\subsection{Discrete derivatives}
We denote by $\matrixIdentity$ the identity matrix. 
The discrete derivatives in 2D are given by
\begin{align}
  \left\{ 
  \begin{array}{ll}
  \mathbb{G} &= 
    \begin{bmatrix}
      \mathcal{D}_1 \otimes \matrixIdentity_2
      \\
      \matrixIdentity_1 \otimes \mathcal{D}_2
    \end{bmatrix}
  \\
  \\
  \pmb{\mathbb{C}} &= 
    \begin{bmatrix}
      \matrixIdentity_1 \otimes \mathcal{D}_2
      \\
    - \mathcal{D}_1 \otimes \matrixIdentity_2
    \end{bmatrix} 
  \\
  \\
  \mathbb{C} &= 
    \begin{bmatrix}
    - \matrixIdentity_1 \otimes \mathcal{D}_2
     & 
      \mathcal{D}_1 \otimes \matrixIdentity_2
    \end{bmatrix} 
  \\
  \\
  \mathbb{D} &= 
    \begin{bmatrix}
      \mathcal{D}_1 \otimes \matrixIdentity_2
     & 
      \matrixIdentity_1 \otimes \mathcal{D}_2
    \end{bmatrix}
  \end{array} \right.
  \label{eq:discrete-derivatives-2d}
\end{align}
The discrete derivatives in 3D are given by
\begin{align}
  \left\{ 
  \begin{array}{ll}
  \mathbb{G} &= 
    \begin{bmatrix}
      \mathcal{D}_1 \otimes \matrixIdentity_2 \otimes \matrixIdentity_3
      \\
      \matrixIdentity_1 \otimes \mathcal{D}_2 \otimes \matrixIdentity_3 
      \\
      \matrixIdentity_1 \otimes \matrixIdentity_2 \otimes \mathcal{D}_3
    \end{bmatrix}
  \\
  \\
  \mathbb{C} &= 
  \begin{bmatrix}
    0    &    - \matrixIdentity_1 \otimes \matrixIdentity_2 \otimes \mathcal{D}_3  &  \matrixIdentity_1 \otimes \mathcal{D}_2 \otimes \matrixIdentity_3 
    \\
    \matrixIdentity_1 \otimes \matrixIdentity_2 \otimes \mathcal{D}_3   &    0   &   - \mathcal{D}_1 \otimes \matrixIdentity_2 \otimes \matrixIdentity_3 
    \\
    - \matrixIdentity_1 \otimes \mathcal{D}_2 \otimes \matrixIdentity_3  & \mathcal{D}_1 \otimes \matrixIdentity_2 \otimes \matrixIdentity_3 & 0 
  \end{bmatrix} 
  \\
  \\
  \mathbb{D} &= 
    \begin{bmatrix}
      \mathcal{D}_1 \otimes \matrixIdentity_2 \otimes \matrixIdentity_3
      & 
      \matrixIdentity_1 \otimes \mathcal{D}_2 \otimes \matrixIdentity_3 
      &
      \matrixIdentity_1 \otimes \matrixIdentity_2 \otimes \mathcal{D}_3
    \end{bmatrix}
  \end{array} \right.
  \label{eq:discrete-derivatives-3d}
\end{align}
\todo{
\begin{remark}
The actual implementation is based on a matrix form, but we should avoid it and implement these operators as functions. This will reduce the memory usage.  
\end{remark}
}
\subsection{The Histopolation Operator}
In the 1D case, the DeRham sequence involves two spaces $V_0 := \Vgrad$ and $V_1 := \Ltwo$. We are interested in the restriction of the histopolation operator to functions in $V_0$. We consider a function $u_h \in V_0$ that is expanded as $u_h := \sum\limits_{1 \le j \le n_{V_0}} u_j \Njone$. 

Let us first define the histopolation matrix form $\mathcal{H}$ as

\subsection{The $\Picurl$ Operator}
\todo{TODO}

In the 2D case, the $\Picurl$ is defined as 
\begin{align}
  P_h^{\textbf{curl}} &=
  \begin{bmatrix}
      \mathcal{H}_1 \otimes \matrixIdentity_2
      \\
      \matrixIdentity_1 \otimes \mathcal{H}_2
  \end{bmatrix}
%  \label{}
\end{align}

In the 3D case, the $\Picurl$ is defined as 
\begin{align}
    \Picurl  & = 
    \begin{bmatrix}
      \mathcal{H}_1 \otimes \matrixIdentity_2 \otimes \matrixIdentity_3
      \\
      \matrixIdentity_1 \otimes \mathcal{H}_2 \otimes \matrixIdentity_3 
      \\
      \matrixIdentity_1 \otimes \matrixIdentity_2 \otimes \mathcal{H}_3 
    \end{bmatrix}
  \label{eq:commuting-projectors-splines-3d}
\end{align}


\subsection{Triangular solver and the symmetric Gauss-Seidel method}
We consider in the following a block matrix 
%
\begin{equation} %\label{eq:matr_A_2d}
\mathcal{A} =\begin{bmatrix}
 A_{11} & A_{12} & A_{13}  \\
 A_{21} & A_{22} & A_{23}  \\
 A_{31} & A_{22} & A_{33}  
\end{bmatrix}.
\end{equation}
%

% ...
\begin{algorithm}[H]
\DontPrintSemicolon
\SetAlgoLined
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{$\mathcal{A}, x, b, \nu_1$}
\Output{$x$}
\BlankLine

  \For{$i \gets 1$ \textbf{to} $\nu_1$} {
    $x \gets x + \texttt{spsolve}(\mathcal{A}, b - \mathcal{A} x, \texttt{lower}=\texttt{True})$ \; 
  }
  \For{$i \gets 1$ \textbf{to} $\nu_1$} {
    $x \gets x + \texttt{spsolve}(\mathcal{A}, b - \mathcal{A} x, \texttt{lower}=\texttt{False})$ \; 
  }

\caption{\texttt{gauss\_seidel}: Symmetric Gauss Seidel solver}
\end{algorithm} 
% ...

% ...
\begin{algorithm}[H]
\DontPrintSemicolon
\SetAlgoLined
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{$\mathcal{A}, b$}
\Output{$x$}
\BlankLine

  $b_1, b_2, b_3 \gets \texttt{unfold}(b)$ \; 
  $x_1 \gets \texttt{spsolve}(A_{11}, b_1, \texttt{lower}=\texttt{True})$                                    \tcp*{Comment}
  $x_2 \gets \texttt{spsolve}(A_{22}, b_2 - A_{21} x_1, \texttt{lower}=\texttt{True})$                       \tcp*{Comment}
  $x_3 \gets \texttt{spsolve}(A_{33}, b_3 - A_{31} x_1 - A_{32} x_2, \texttt{lower}=\texttt{True})$          \tcp*{Comment}
  $x \gets \texttt{fold}(x_1, x_2, x_3)$ \; 

\caption{\texttt{spsolve}: Triangular solver for lower block matrix}
\end{algorithm} 
% ...

% ...
\begin{algorithm}[H]
\DontPrintSemicolon
\SetAlgoLined
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{$\mathcal{A}, b$}
\Output{$x$}
\BlankLine

  $b_1, b_2, b_3 \gets \texttt{unfold}(b)$ \; 
  $x_3 \gets \texttt{spsolve}(A_{33}, b_3, \texttt{lower}=\texttt{False})$                                    \tcp*{Comment}
  $x_2 \gets \texttt{spsolve}(A_{22}, b_2 - A_{23} x_3, \texttt{lower}=\texttt{False})$                       \tcp*{Comment}
  $x_1 \gets \texttt{spsolve}(A_{11}, b_1 - A_{12} x_2 - A_{13} x_3, \texttt{lower}=\texttt{False})$          \tcp*{Comment}
  $x \gets \texttt{fold}(x_1, x_2, x_3)$ \; 

\caption{\texttt{spsolve}: Triangular solver for upper block matrix}
\end{algorithm} 
% ...

Since the diagonal block matrices can be either a Kronecker product of 3 matrices or the sum a Kronecker product of 3 matrices, we can then derive efficient matrix-free implementation as described in the following algorithms.

% ...
\begin{algorithm}[ht]
\DontPrintSemicolon
\SetAlgoLined
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{$\mathcal{A} = A_1 \otimes A_2 \otimes A_3, b$}
\Output{$x$}
\BlankLine

  \For{$i_1 \gets 1$ \textbf{to} $n_1$} {
    \For{$i_2 \gets 1$ \textbf{to} $n_2$} {
      \For{$i_3 \gets 1$ \textbf{to} $n_3$} {
        $i \gets \texttt{multi\_index}(i_1, i_2, i_3)$ \; 
        $r \gets 0$ \; 
        $a_d \gets 1$ \; 

        \For{$k_1 \gets A_1.\texttt{ptr}[i_1]$ \textbf{to} $A_1.\texttt{ptr}[i_1+1] - 1$} {
          $j_1 \gets A_1.\texttt{indices}[k_1]$ \; 
          $a_1 \gets A_1.\texttt{data}[k_1]$ \; 
          \For{$k_2 \gets A_2.\texttt{ptr}[i_2]$ \textbf{to} $A_2.\texttt{ptr}[i_2+1] - 1$} {
            $j_2 \gets A_2.\texttt{indices}[k_2]$ \; 
            $a_2 \gets A_2.\texttt{data}[k_2]$ \; 
            \For{$k_3 \gets A_3.\texttt{ptr}[i_3]$ \textbf{to} $A_3.\texttt{ptr}[i_3+1] - 1$} {
              $j_3 \gets A_3.\texttt{indices}[k_3]$ \; 
              $a_3 \gets A_3.\texttt{data}[k_3]$ \; 
              $j \gets \texttt{multi\_index}(j_1, j_2, j_3)$ \; 
              \uIf{$i < j$}{
                $r \gets r + a_1 a_2 a_3 x[j]$ \; 
              }
              \Else{
                $a_d \gets a_1 a_2 a_3$ \; 
              }
            }
          }
        }
        $x[i] \gets \frac{1}{a_d}(b[i] - r)$ \; 
      }
    }
  }

\caption{\texttt{spsolve}: Triangular solver for lower Kronecker product matrix, for CSR storage.}
\end{algorithm} 
% ...


% ...
\begin{algorithm}[ht]
\DontPrintSemicolon
\SetAlgoLined
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{$\mathcal{A} = A_1 \otimes A_2 \otimes A_3, b$}
\Output{$x$}
\BlankLine

  \For{$i_1 \gets 1$ \textbf{to} $n_1$} {
    \For{$i_2 \gets 1$ \textbf{to} $n_2$} {
      \For{$i_3 \gets 1$ \textbf{to} $n_3$} {
        $i \gets \texttt{multi\_index}(i_1, i_2, i_3)$ \; 
        $r \gets 0$ \; 
        $a_d \gets 1$ \; 

        \For{$k_1 \gets A_1.\texttt{ptr}[i_1]$ \textbf{to} $A_1.\texttt{ptr}[i_1+1] - 1$} {
          $j_1 \gets A_1.\texttt{indices}[k_1]$ \; 
          $a_1 \gets A_1.\texttt{data}[k_1]$ \; 
          \For{$k_2 \gets A_2.\texttt{ptr}[i_2]$ \textbf{to} $A_2.\texttt{ptr}[i_2+1] - 1$} {
            $j_2 \gets A_2.\texttt{indices}[k_2]$ \; 
            $a_2 \gets A_2.\texttt{data}[k_2]$ \; 
            \For{$k_3 \gets A_3.\texttt{ptr}[i_3]$ \textbf{to} $A_3.\texttt{ptr}[i_3+1] - 1$} {
              $j_3 \gets A_3.\texttt{indices}[k_3]$ \; 
              $a_3 \gets A_3.\texttt{data}[k_3]$ \; 
              $j \gets \texttt{multi\_index}(j_1, j_2, j_3)$ \; 
              \If{$i \ge j$}{
                $r \gets r + a_1 a_2 a_3 x[j]$ \; 
              }
              \If{$i = j$}{
                $a_d \gets a_1 a_2 a_3$ \; 
              }
            }
          }
        }
        $x[i] \gets \frac{1}{a_d}(b[i] - r)$ \; 
      }
    }
  }

\caption{\texttt{spsolve}: Triangular solver for upper Kronecker product matrix, for CSR storage.}
\end{algorithm} 
% ...

\subsection{Computational Cost}
