\section{Kronecker algebra}
\label{sec:produit_kronecker_sec}
In this section, we present an overview about an interesting subject, which is the Kronecker Algebra, and which will be of a big interest in the Fast-IGA approach. More details about Kronecker Algebra can be found in \cite{vanloan2000,Graham_book,Bernstein_book}.

\begin{definition}[The $\mathbf{vec}$ operator]
Let $A=(a_{ij}) \in \mathcal{M}_{n \times m}$, the $\mathbf{vec}$ operator is defined as,
\begin{align}
\mathbf{vec} A = \left(\begin{array}{c}
 A_{:,1}
\\
\vdots
\\
 A_{:,m}
\end{array}\right) \in \mathbb{R}^{mn}
\end{align}
which is simply a vector composed by stacking all the columns of $A$. Where we denote $ A_{:,j}$ the $j^{th}$ column of $A$.
\\
We also define the inverse operator of $\mathbf{vec}$ by,
\begin{align}
A = \mathbf{vec}^{-1} \mathbf{vec} A
\end{align}
\end{definition}

\begin{definition}[Kronecker product]
Let $A=(a_{ij}) \in \mathcal{M}_{m \times n}$ and $B=(b_{ij}) \in \mathcal{M}_{r \times s}$ be two matrices. The Kronecker product of $A$ and $B$, denoted by $A \otimes B  \in \mathcal{M}_{mr \times ns}$, defines the following matrix:
\begin{align}
A \otimes B = 
\left(\begin{array}{cccc}
a_{11}B & a_{12}B & \cdots & a_{1n}B 
\\
a_{21}B & a_{22}B & \cdots & a_{2n}B  
\\
\vdots & \vdots &  & \vdots 
\\
a_{m1}B & a_{m2}B & \cdots & a_{mn}B 
\end{array}\right)
\end{align}
\end{definition}

\subsubsection*{Example}
Let 
\begin{align*}
A = 
\left(\begin{array}{cc}
a_{11} & a_{12}
\\
a_{21} & a_{22}
\end{array}\right),~~~
B = 
\left(\begin{array}{cc}
b_{11} & b_{12}
\\
b_{21} & b_{22}
\end{array}\right)
\end{align*}
then their Kronecker product is,
\begin{align}
A \otimes B = 
\left(\begin{array}{cccc}
a_{11}b_{11} & a_{11}b_{12} & a_{12}b_{11} & a_{12}b_{12}
\\
a_{11}b_{21} & a_{11}b_{22} & a_{12}b_{21} & a_{12}b_{22}
\\
a_{21}b_{11} & a_{21}b_{12} & a_{22}b_{11} & a_{22}b_{12}
\\
a_{21}b_{21} & a_{21}b_{22} & a_{22}b_{21} & a_{22}b_{22}
\end{array}\right)
\end{align}


\subsubsection*{Properties}
\begin{proposition}
If $\alpha$ is a scalar, then 
\begin{align}
A \otimes \alpha B = \alpha A \otimes B
\end{align}
\end{proposition}

\begin{proposition}
We have,
\begin{align}
( A + B ) \otimes C &= A \otimes C + B \otimes C
\\
A \otimes ( B + C ) &= A \otimes B + A \otimes C
\end{align}
\end{proposition}

\begin{proposition}[Associativity]
\begin{align}
A \otimes B \otimes C = A \otimes ( B \otimes C ) = ( A \otimes B ) \otimes C
\end{align}
\end{proposition}

\begin{proposition}[Mixed Product Rule]
\begin{align}
( A \otimes B ) ( C \otimes D ) = AC \otimes BD
\end{align}
and,
\begin{align}
( A \otimes B ) ^p = A^p \otimes B^p,~~~\forall p \in \mathbb{N}
\end{align}
\end{proposition}

\begin{proposition}
\begin{align}
( A \otimes B )^T = A^T \otimes B^T
\end{align}
\end{proposition}

\begin{proposition}
\begin{align}
( A \otimes B )^{-1} = A^{-1} \otimes B^{-1}
\end{align}
\end{proposition}

\begin{proposition}
\begin{align}
\label{kronecker_vec_abc}
\mathbf{vec} ( ABC ) = ( C^T \otimes A ) \mathbf{vec} ( B )
\end{align}
\end{proposition}

\begin{proposition}
\begin{align}
\mathbf{tr} ( A \otimes B ) = \mathbf{tr} ( B \otimes A ) =  \mathbf{tr} ( A ) \mathbf{tr} ( B )
\end{align}
\end{proposition}

\begin{proposition}
Let $A \in \mathcal{M}_{n \times n}$ and $B \in \mathcal{M}_{m \times m}$, we have,
\begin{align}
\label{kronecker_prod_spec}
\mathbf{mspec} ( A \otimes B ) = \{ \lambda \mu,~~\lambda \in \mathbf{mspec}(A),~\mu \in \mathbf{mspec}(B) \}
\end{align}
\end{proposition}

%\begin{proposition}
%Let $A \in \mathcal{M}_{n \times n}$ and $B \in \mathcal{M}_{m \times m}$, we have,
%\begin{align}
%\mathbf{det} ( A \otimes B ) = ( \mathbf{det} A )^m ( \mathbf{det} B )^n
%\end{align}
%\end{proposition}
%We deduce from \ref{kronecker_prod_spec},
%\begin{proposition}
%Let $A \in \mathcal{M}_{n \times n}$, we have,
%\begin{align}
%\rho ( A \otimes A ) = \rho ( A )^2
%\end{align}
%\end{proposition}

%\begin{proposition}
%Let $f$ be an analytic function, $A \in \mathcal{M}_{n \times n}$ such that $f(A)$ exists, then we have,
%\begin{align}
%f(I_m \otimes A) = I_m \otimes f(A)
%\\
%f( A \otimes I_m ) = f(A) \otimes I_m 
%\end{align}
%\end{proposition}

%\begin{proposition}
%Let $X \in \mathbb{R}^{n}$ and $Y \in \mathbb{R}^{m}$, be two vectors. We have,
%\begin{align}
%X Y^T = X \otimes (Y^T) = (Y^T) \otimes X
%\end{align}
%moreover, we have,
%\begin{align}
%\mathbf{vec} (XY^T) = Y \otimes X 
%\end{align}
%\end{proposition}
%
%\begin{definition}[Kronecker permutation matrix]
%The Kronecker permutation matrix $P_{n,m} \in  \mathcal{M}_{nm \times nm}$, is defined by,
%\begin{align}
%P_{n,m} = \sum_{i,j=1}^{n,m} E_{i,j,n \times m} \otimes E_{j,i,m \times n}
%\end{align}
%\end{definition}

%\begin{proposition}
%Let $A \in \mathcal{M}_{m \times n}$, we have,
%\begin{align}
%\mathbf{vec} (A^T) = P_{m,n} \mathbf{vec} (A) 
%\end{align}
%\end{proposition}
%
%\begin{proposition}
%Let us consider the Kronecker permutation matrix $P_{n,m} \in  \mathcal{M}_{nm \times nm}$. Then we have,
%\begin{itemize}
%\item $P_{n,m}^T=P_{n,m}^{-1}=P_{m,n}$
%\item $P_{n,m}$ is orthogonal,
%\item $P_{n,m} P_{m,n} =I_{nm}$
%\item $P_{n,n}$ is orthogonal, symmetric and involutory,
%\item $P_{n,n}$ is a reflector,
%\item $\mathbf{tr} P_{n,n} = n$,
%\item $P_{1,m}=I_{m}$, and $P_{n,1}=I_{n}$
%\item if $X \in \mathbb{R}^{n}$ and $Y \in \mathbb{R}^{m}$, then,
%\begin{align}
%P_{n,m} ( Y \otimes X ) = X \otimes Y
%\end{align}
%\item if $A \in \mathcal{M}_{n \times m}$ and $B \in \mathcal{M}_{r \times s}$, then
%\begin{align}
%P_{r,n} ( A \otimes B ) P_{m,s} = B \otimes A
%\end{align}
%\item if $A \in \mathcal{M}_{n \times n}$ and $B \in \mathcal{M}_{m \times m}$, then
%\begin{align}
%P_{m,n} ( A \otimes B ) P_{n,m} =  P_{m,n} ( A \otimes B ) P_{m,n}^{-1} = B \otimes A
%\end{align}
%Therefor, $A \otimes B$ and $B \otimes A$ are similar.
%\end{itemize}
%\end{proposition}

\begin{proposition}
Let $A \in \mathcal{M}_{n \times n}$ and $B \in \mathcal{M}_{m \times m}$, then we have the following properties,
\begin{itemize}
\item if $A$ and $B$ are diagonal, then $A\otimes B$ is diagonal,
\item if $A$ and $B$ are upper triangular, then $A\otimes B$ is upper triangular,
\item if $A$ and $B$ are lower triangular, then $A\otimes B$ is lower triangular,
\end{itemize}
\end{proposition}

%\begin{proposition}
%Let $A,C \in \mathcal{M}_{n \times m}$ and $B,D \in \mathcal{M}_{r \times s}$. If $A$ is (left equivalent, right equivalent, equivalent) to $C$, and assume that $B$ is (left equivalent, right equivalent, equivalent) to $D$. Then, $A \otimes B$ is (left equivalent, right equivalent, equivalent) to $C \otimes D$.
%\end{proposition}

%\begin{remark}
%The use of Kronecker product preconditioners is well known \cite{vanloan,Langville_Stewart,Elisabeth_Ullmann,GRIGORI:2008:INRIA-00268301:5}, it is based on results of the form,
%\begin{align} 
%\mbox{Minimizing,}
%~~~~~~~~
%\phi_A(B,C) = \| A - B \otimes C  \|^2
%\end{align}
%for a chosen norm.
%\end{remark}

\subsection{Kronecker sum}
\begin{definition}[Kronecker sum]
Let $A=(a_{ij}) \in \mathcal{M}_{n \times n}$ and $B=(b_{ij}) \in \mathcal{M}_{m \times m}$ be two matrices. The Kronecker sum of $A$ and $B$, denoted by $A \oplus B  \in \mathcal{M}_{mn \times mn}$, defines the following matrix:
\begin{align}
A \oplus B = A \otimes I_m + I_n \otimes B
\end{align}
\end{definition}

\begin{proposition}
Let $A \in \mathcal{M}_{n \times n}$ and $B \in \mathcal{M}_{m \times m}$, we have,
\begin{align}
\label{kronecker_sum_spec}
\mathbf{mspec} ( A \oplus B ) = \{ \lambda +  \mu,~~\lambda \in \mathbf{mspec}(A),~\mu \in \mathbf{mspec}(B) \}
\end{align}
\end{proposition}

\subsection{Solving $AX+XB=C$}
Let $A \in \mathcal{M}_{n \times n}$, $B \in \mathcal{M}_{m \times m}$ and $C \in \mathcal{M}_{n \times m}$. The aim of this section, is to solve the equation:
\begin{align}
\label{kronecker_eq1}
AX+XB=C
\end{align}
we can rewrite this equation in term of the Kronecker sum:
\begin{align}
(B^T \oplus A)\mathbf{vec}X =\mathbf{vec}C
\end{align}
or equivalently,
\begin{align}
G x = c
\end{align}
where,
$G = (B^T \oplus A)$, $x = \mathbf{vec}X $, and $c = \mathbf{vec}C$.
\\
Using the property \ref{kronecker_sum_spec}, we can easily check that \ref{kronecker_eq1} has a unique solution if and only if $G$ is nonsingular, \textit{i.e} $\lambda +  \mu \neq 0,~~\forall \lambda \in \mathbf{mspec}(A),~\forall \mu \in \mathbf{mspec}(B)$, which can be written in the form,
\begin{align}
\mathbf{mspec}(A) \cap \mathbf{mspec}(-B) = \emptyset
\end{align}

\begin{proposition}
If $\mathbf{mspec}(A) \cap \mathbf{mspec}(-B) = \emptyset$, then there exists a unique matrix $X \in \mathcal{M}_{n \times m}$, satisfying \ref{kronecker_eq1}. Moreover, the matrices $\left(\begin{array}{cc}
A & C
\\
0 & -B
\end{array}\right)$ and $\left(\begin{array}{cc}
A & 0
\\
0 & -B
\end{array}\right)$ are similar and verify,
\begin{align}
\left(\begin{array}{cc}
A & C
\\
0 & -B
\end{array}\right) = \left(\begin{array}{cc}
I & X
\\
0 & I
\end{array}\right)
\left(\begin{array}{cc}
A & 0
\\
0 & -B
\end{array}\right)
\left(\begin{array}{cc}
I & -X
\\
0 & I
\end{array}\right).
\end{align}
\end{proposition}

\subsection{Solving $AXB=C$}
Let $A,B,C$ and $X \in \mathcal{M}_{n \times n}$. As seen previously, using \ref{kronecker_vec_abc}, the equation 
\begin{align}
\label{kronecker_eq2}
AXB=C
\end{align}
can be written in the form,
\begin{align}
H x = c
\end{align}
where,
$H = (B^T \otimes A)$, $x = \mathbf{vec}X $, and $c = \mathbf{vec}C$.
\\
Using the property \ref{kronecker_prod_spec}, we can easily check that \ref{kronecker_eq2} has a unique solution if and only if $H$ is nonsingular, \textit{i.e} $\lambda  \mu \neq 0,~~\forall \lambda \in \mathbf{mspec}(A),~\forall \mu \in \mathbf{mspec}(B)$, which is equivalent to, $A$ and $B$ are both nonsingular.

%\subsection{Solving $\sum_{i=1}^r A_i X B_i=C$}
%Let $A_i,B_i,C,~~1 \leq i \leq r$ and $X \in \mathcal{M}_{n \times n}$. Using, the previous result, it is easy to show that the solution of:
%\begin{align}
%\label{kronecker_eq3}
%\sum_{i=1}^r A_i X B_i=C
%\end{align}
%can be written in the form,
%\begin{align}
%H x = c
%\end{align}
%where,
%$H = \sum_{i=1}^r (B_i^T \otimes A_i)$, $x = \mathbf{vec}X $, and $c = \mathbf{vec}C$.

\subsection{Algorithms}
\subsubsection{Fast Diagonalization method}
In order to device a fast solver the Poisson and Laplace problems, we choose to follow the work of Sangalli and Tani \cite{sangalli2016}, we describe in the sequel the fast diagonalization method in the case of Isogeometric Analysis. This method was first introduced in \cite{lynch1964}.
\\
For the sack of simplicity, we shall consider the following Laplace problem, 
\begin{equation}
  \begin{cases}
  - \nabla^2 u + \tau u = f, \quad \Omega \\ 
  u=0, \quad \partial\Omega
  \end{cases}
  \label{eq:laplace}
\end{equation}
The Poisson problem and its solver shall be retrieved with $\tau=0$.
After discretizing the Laplace problem, we get the following linear system
\begin{equation}
  \mathcal{L}_{\tau} x := \left( K_1 \otimes M_2 \otimes M_3 +  M_1 \otimes K_2 \otimes M_3 + M_1 \otimes M_2 \otimes K_3 + \tau M_1 \otimes M_2 \otimes M_3 \right) x = b 
  \label{eq:laplace-kron}
\end{equation}
We first consider the generalized eigendecompositions problems
\begin{equation}
  K_1 U_1 = M_1 U_1 D_1, \quad  
  K_2 U_2 = M_2 U_2 D_2, \quad 
  K_3 U_3 = M_3 U_3 D_3,
  \label{eq:generalized-eigen-decomp}
\end{equation}
where $D_1$, $D_2$ and $D_3$ are diagonal matrices such that
\begin{equation}
  U_1^T M_1 U_1 = I_1, \quad  
  U_2^T M_2 U_2 = I_2, \quad  
  U_3^T M_3 U_3 = I_3
%  \label{}
\end{equation}
Therefor, (\ref{eq:laplace-kron}) can be written as
\begin{equation}
  \left( U_1 \otimes U_2 \otimes U_3 \right)^{-1} 
  \left( D_1 \otimes I_2 \otimes I_3 +  I_1 \otimes D_2 \otimes I_3 + I_1 \otimes I_2 \otimes D_3 + \tau I_1 \otimes I_2 \otimes I_3 \right) 
  \left( U_1 \otimes U_2 \otimes U_3 \right)^{-T} 
  x = b 
  \label{eq:laplace-kron-fact}
\end{equation}
The direct solver for the Laplace problem (\ref{eq:laplace-kron}), is then given by the following algorithm, where we omit the initialization step achieved by solving the generalized eigendecompositions in (\ref{eq:generalized-eigen-decomp}):
% ...
\begin{algorithm}[ht]
\DontPrintSemicolon
\SetAlgoLined
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{$\mathcal{L}_{\tau}, b$}
\Output{$x$}
\BlankLine
$\tilde{b} \gets \left( U_1 \otimes U_2 \otimes U_3 \right) b$ \; 
$\tilde{x} \gets \left( D_1 \otimes I_2 \otimes I_3 +  I_1 \otimes D_2 \otimes I_3 + I_1 \otimes I_2 \otimes D_3 + \tau I_1 \otimes I_2 \otimes I_3 \right)^{-1} \tilde{b}$ \; 
$x \gets \left( U_1 \otimes U_2 \otimes U_3 \right)^T \tilde{x}$ \; 

\caption{\texttt{fast\_diag}: Fast diagonalization method for Laplace problem}
\end{algorithm} 
% ...
We consider now the vector Laplace problem
\begin{equation}
  \begin{cases}
  - \nabla^2 \mathbf{u} + \tau \mathbf{u} = \mathbf{f}, \quad \Omega \\
  \mathbf{u}=0, \quad \partial\Omega
  \end{cases}
  \label{eq:laplace-vector}
\end{equation}
Which can be written in a matrix form as
\begin{equation} 
  \bm{\mathcal{L}}_{\tau} =
  \begin{bmatrix}
   \mathcal{L}_{\tau} &                  0 & 0  \\
                    0 & \mathcal{L}_{\tau} & 0  \\
                    0 &                  0 & \mathcal{L}_{\tau}  
  \end{bmatrix}.
  \label{eq:laplace-vector-matrix-form}
\end{equation}
Therefor, a fast solver for the vector Laplace problem (\ref{eq:laplace-vector}) can be given by the following algorithm
% ...
\begin{algorithm}[H]
\DontPrintSemicolon
\SetAlgoLined
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{$\bm{\mathcal{L}}_{\tau}, b$}
\Output{$x$}
\BlankLine

  $b_1, b_2, b_3 \gets \texttt{unfold}(b)$ \; 
  $x_1 \gets \texttt{fast\_diag}(\mathcal{L}_{\tau}, b_1)$ \; 
  $x_2 \gets \texttt{fast\_diag}(\mathcal{L}_{\tau}, b_2)$ \; 
  $x_3 \gets \texttt{fast\_diag}(\mathcal{L}_{\tau}, b_3)$ \; 
  $x \gets \texttt{fold}(x_1, x_2, x_3)$ \; 

\caption{\texttt{fast\_diag}: Fast diagonalization method for the vector Laplace problem}
\end{algorithm} 
% ...


\subsubsection{Triangular solver and the symmetric Gauss-Seidel method}
We consider in the following a block matrix 
%
\begin{equation} %\label{eq:matr_A_2d}
\mathcal{A} =\begin{bmatrix}
 A_{11} & A_{12} & A_{13}  \\
 A_{21} & A_{22} & A_{23}  \\
 A_{31} & A_{22} & A_{33}  
\end{bmatrix}.
\end{equation}
%

% ...
\begin{algorithm}[H]
\DontPrintSemicolon
\SetAlgoLined
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{$\mathcal{A}, x, b, \nu_1$}
\Output{$x$}
\BlankLine

  \For{$i \gets 1$ \textbf{to} $\nu_1$} {
    $x \gets x + \texttt{spsolve}(\mathcal{A}, b - \mathcal{A} x, \texttt{lower}=\texttt{True})$ \; 
  }
  \For{$i \gets 1$ \textbf{to} $\nu_1$} {
    $x \gets x + \texttt{spsolve}(\mathcal{A}, b - \mathcal{A} x, \texttt{lower}=\texttt{False})$ \; 
  }

\caption{\texttt{gauss\_seidel}: Symmetric Gauss Seidel solver}
\end{algorithm} 
% ...

% ...
\begin{algorithm}[H]
\DontPrintSemicolon
\SetAlgoLined
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{$\mathcal{A}, b$}
\Output{$x$}
\BlankLine

  $b_1, b_2, b_3 \gets \texttt{unfold}(b)$ \; 
  $x_1 \gets \texttt{spsolve}(A_{11}, b_1, \texttt{lower}=\texttt{True})$                                    \tcp*{Comment}
  $x_2 \gets \texttt{spsolve}(A_{22}, b_2 - A_{21} x_1, \texttt{lower}=\texttt{True})$                       \tcp*{Comment}
  $x_3 \gets \texttt{spsolve}(A_{33}, b_3 - A_{31} x_1 - A_{32} x_2, \texttt{lower}=\texttt{True})$          \tcp*{Comment}
  $x \gets \texttt{fold}(x_1, x_2, x_3)$ \; 

\caption{\texttt{spsolve}: Triangular solver for lower block matrix}
\end{algorithm} 
% ...

% ...
\begin{algorithm}[H]
\DontPrintSemicolon
\SetAlgoLined
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{$\mathcal{A}, b$}
\Output{$x$}
\BlankLine

  $b_1, b_2, b_3 \gets \texttt{unfold}(b)$ \; 
  $x_3 \gets \texttt{spsolve}(A_{33}, b_3, \texttt{lower}=\texttt{False})$                                    \tcp*{Comment}
  $x_2 \gets \texttt{spsolve}(A_{22}, b_2 - A_{23} x_3, \texttt{lower}=\texttt{False})$                       \tcp*{Comment}
  $x_1 \gets \texttt{spsolve}(A_{11}, b_1 - A_{12} x_2 - A_{13} x_3, \texttt{lower}=\texttt{False})$          \tcp*{Comment}
  $x \gets \texttt{fold}(x_1, x_2, x_3)$ \; 

\caption{\texttt{spsolve}: Triangular solver for upper block matrix}
\end{algorithm} 
% ...

Since the diagonal block matrices can be either a Kronecker product of 3 matrices or the sum a Kronecker product of 3 matrices, we can then derive efficient matrix-free implementation as described in the following algorithms.

% ...
\begin{algorithm}[ht]
\DontPrintSemicolon
\SetAlgoLined
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{$\mathcal{A} = A_1 \otimes A_2 \otimes A_3, b$}
\Output{$x$}
\BlankLine

  \For{$i_1 \gets 1$ \textbf{to} $n_1$} {
    \For{$i_2 \gets 1$ \textbf{to} $n_2$} {
      \For{$i_3 \gets 1$ \textbf{to} $n_3$} {
        $i \gets \texttt{multi\_index}(i_1, i_2, i_3)$ \; 
        $r \gets 0$ \; 
        $a_d \gets 1$ \; 

        \For{$k_1 \gets A_1.\texttt{ptr}[i_1]$ \textbf{to} $A_1.\texttt{ptr}[i_1+1] - 1$} {
          $j_1 \gets A_1.\texttt{indices}[k_1]$ \; 
          $a_1 \gets A_1.\texttt{data}[k_1]$ \; 
          \For{$k_2 \gets A_2.\texttt{ptr}[i_2]$ \textbf{to} $A_2.\texttt{ptr}[i_2+1] - 1$} {
            $j_2 \gets A_2.\texttt{indices}[k_2]$ \; 
            $a_2 \gets A_2.\texttt{data}[k_2]$ \; 
            \For{$k_3 \gets A_3.\texttt{ptr}[i_3]$ \textbf{to} $A_3.\texttt{ptr}[i_3+1] - 1$} {
              $j_3 \gets A_3.\texttt{indices}[k_3]$ \; 
              $a_3 \gets A_3.\texttt{data}[k_3]$ \; 
              $j \gets \texttt{multi\_index}(j_1, j_2, j_3)$ \; 
              \uIf{$i < j$}{
                $r \gets r + a_1 a_2 a_3 x[j]$ \; 
              }
              \Else{
                $a_d \gets a_1 a_2 a_3$ \; 
              }
            }
          }
        }
        $x[i] \gets \frac{1}{a_d}(b[i] - r)$ \; 
      }
    }
  }

\caption{\texttt{spsolve}: Triangular solver for lower Kronecker product matrix, for CSR storage.}
\end{algorithm} 
% ...


% ...
\begin{algorithm}[ht]
\DontPrintSemicolon
\SetAlgoLined
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{$\mathcal{A} = A_1 \otimes A_2 \otimes A_3, b$}
\Output{$x$}
\BlankLine

  \For{$i_1 \gets 1$ \textbf{to} $n_1$} {
    \For{$i_2 \gets 1$ \textbf{to} $n_2$} {
      \For{$i_3 \gets 1$ \textbf{to} $n_3$} {
        $i \gets \texttt{multi\_index}(i_1, i_2, i_3)$ \; 
        $r \gets 0$ \; 
        $a_d \gets 1$ \; 

        \For{$k_1 \gets A_1.\texttt{ptr}[i_1]$ \textbf{to} $A_1.\texttt{ptr}[i_1+1] - 1$} {
          $j_1 \gets A_1.\texttt{indices}[k_1]$ \; 
          $a_1 \gets A_1.\texttt{data}[k_1]$ \; 
          \For{$k_2 \gets A_2.\texttt{ptr}[i_2]$ \textbf{to} $A_2.\texttt{ptr}[i_2+1] - 1$} {
            $j_2 \gets A_2.\texttt{indices}[k_2]$ \; 
            $a_2 \gets A_2.\texttt{data}[k_2]$ \; 
            \For{$k_3 \gets A_3.\texttt{ptr}[i_3]$ \textbf{to} $A_3.\texttt{ptr}[i_3+1] - 1$} {
              $j_3 \gets A_3.\texttt{indices}[k_3]$ \; 
              $a_3 \gets A_3.\texttt{data}[k_3]$ \; 
              $j \gets \texttt{multi\_index}(j_1, j_2, j_3)$ \; 
              \If{$i \ge j$}{
                $r \gets r + a_1 a_2 a_3 x[j]$ \; 
              }
              \If{$i = j$}{
                $a_d \gets a_1 a_2 a_3$ \; 
              }
            }
          }
        }
        $x[i] \gets \frac{1}{a_d}(b[i] - r)$ \; 
      }
    }
  }

\caption{\texttt{spsolve}: Triangular solver for upper Kronecker product matrix, for CSR storage.}
\end{algorithm} 
% ...

\subsection{Computational Cost}
